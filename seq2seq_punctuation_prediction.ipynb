{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "seq2seq punctuation prediction.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XDniUtS-ZSUA",
        "colab_type": "text"
      },
      "source": [
        "This notebook to train sequence to sequence model for Arabic punctuation prediction. This project followed the [Neural Machine Translation]((https://www.tensorflow.org/tutorials/text/nmt_with_attention)) which available from TensorFlow.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k3tpNu5xWvCp",
        "colab_type": "text"
      },
      "source": [
        "### Import necessary libraries\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "GpdedrlzyueD",
        "colab": {}
      },
      "source": [
        "from __future__ import absolute_import, division, print_function, unicode_literals\n",
        "\n",
        "import tensorflow as tf\n",
        "tf.enable_eager_execution()\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.ticker as ticker\n",
        "from sklearn.utils import shuffle\n",
        "\n",
        "import re\n",
        "import numpy as np\n",
        "import os\n",
        "import time\n",
        "import pandas as pd"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AgKjDChHXLe7",
        "colab_type": "text"
      },
      "source": [
        "## Prepare data\n",
        "In this step, we convert input and output files to numeric sequence with length 10 as the maximum length."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "PWulGtUEyueH",
        "colab": {}
      },
      "source": [
        "def preprocess_sentence(file_name):\n",
        "    '''\n",
        "    DESCRIPTION:\n",
        "    This function to read the file, then add <start> and <end> as a tag in begin and end for each sequence.\n",
        "    INPUT: \n",
        "    file_name: file name\n",
        "    OUTPUT: \n",
        "    text: text after preprocess\n",
        "    ''' \n",
        "    with open(file_name , 'r', encoding='windows-1256') as f:\n",
        "        text = f.readlines()\n",
        "\n",
        "    text = ['<start> '+ t.replace('\\n','') + ' <end>' for t in text]\n",
        "    return text\n",
        "\n",
        "\n",
        "def calculate_max_length(tensor):\n",
        "    '''\n",
        "    DESCRIPTION:\n",
        "    This function to Calculates the max length in tensor\n",
        "    INPUT: \n",
        "    tensor: input tensor\n",
        "    OUTPUT: \n",
        "    _: max length of tensor\n",
        "    ''' \n",
        "    return max(len(t) for t in tensor)\n",
        "\n",
        "def tokenize(text):\n",
        "    '''\n",
        "    DESCRIPTION:\n",
        "    This function to convert inputs to numeric sequences with the maximum length\n",
        "    INPUT: \n",
        "    text: list of string\n",
        "    OUTPUT: \n",
        "    tokenizer: object of converted text into a sequence of integer\n",
        "    text_vector: vector of converted text into a sequence of integer\n",
        "    max_length: max length in text_vector\n",
        "    ''' \n",
        "\n",
        "    # Choose the top 9000 words from the vocabulary\n",
        "    top_k = 9000\n",
        "    tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=top_k, oov_token=\"<unk>\",filters='')\n",
        "    tokenizer.fit_on_texts(text)\n",
        "    train_seqs = tokenizer.texts_to_sequences(text)\n",
        "\n",
        "    #add a word for padding \n",
        "    tokenizer.word_index['<pad>'] = 0\n",
        "    tokenizer.index_word[0] = '<pad>'\n",
        "\n",
        "    # Create the tokenized vectors\n",
        "    text_seqs = tokenizer.texts_to_sequences(text)\n",
        "\n",
        "    # Pad each vector to the max_length of the vector\n",
        "    text_vector = tf.keras.preprocessing.sequence.pad_sequences(text_seqs, padding='post')\n",
        "\n",
        "    # Calculates the max_length, which is used to store the attention weights\n",
        "    max_length = calculate_max_length(text_seqs)\n",
        "    \n",
        "    return tokenizer, text_vector,max_length"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "EJVB3qG1yueL",
        "colab": {}
      },
      "source": [
        "# prepare input data\n",
        "input = preprocess_sentence('input_punctuation.txt')\n",
        "input_tokenizer, input_tensor ,input_max_length = tokenize(input)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "XfOxkC0dyueO",
        "colab": {}
      },
      "source": [
        "# prepare output data\n",
        "target = preprocess_sentence('output_punctuation.txt')\n",
        "target_tokenizer, target_tensor ,target_max_length = tokenize(target)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ykixCCr20Hjg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "eb572d5c-3fea-4eed-ff2b-006ef9065814"
      },
      "source": [
        "# data shapes\n",
        "(input_tensor.shape),(target_tensor.shape) "
      ],
      "execution_count": 510,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((628500, 12), (628500, 12))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 510
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "a38_ZKX6yueR",
        "outputId": "c1ae9362-8d52-4ba4-a5cc-3321aea52ede",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 454
        }
      },
      "source": [
        "def convert(tokenizer, tensor):\n",
        "    '''\n",
        "    DESCRIPTION:\n",
        "    This function to convert index to word for input tensor\n",
        "    INPUT: \n",
        "    tokenizer: object of converted text into a sequence of integer\n",
        "    tensor: list of integer\n",
        "    OUTPUT: \n",
        "    None\n",
        "    ''' \n",
        "    for t in tensor:\n",
        "        if t!=0:\n",
        "              print (\"%d ----> %s\" % (t, tokenizer.index_word[t]))\n",
        "    \n",
        "print (\"Input Language; index to word mapping\")\n",
        "convert(input_tokenizer, input_tensor[0])  \n",
        "print (\"Output Language; index to word mapping\")\n",
        "convert(target_tokenizer, target_tensor[0]) "
      ],
      "execution_count": 221,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input Language; index to word mapping\n",
            "2 ----> <start>\n",
            "2368 ----> تم\n",
            "1 ----> <unk>\n",
            "39 ----> هذا\n",
            "1 ----> <unk>\n",
            "1 ----> <unk>\n",
            "4048 ----> بواسطة\n",
            "1 ----> <unk>\n",
            "1 ----> <unk>\n",
            "512 ----> الكتاب\n",
            "87 ----> شرح\n",
            "3 ----> <end>\n",
            "Output Language; index to word mapping\n",
            "3 ----> <start>\n",
            "2 ----> space\n",
            "2 ----> space\n",
            "2 ----> space\n",
            "2 ----> space\n",
            "2 ----> space\n",
            "2 ----> space\n",
            "2 ----> space\n",
            "2 ----> space\n",
            "7 ----> :\n",
            "2 ----> space\n",
            "4 ----> <end>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kZG-cQ4genhn",
        "colab_type": "text"
      },
      "source": [
        "### Data Visualization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PkEKybqyNghP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 284
        },
        "outputId": "3ba9aabe-5aae-4980-eccf-a7197ace2e61"
      },
      "source": [
        "target = pd.DataFrame(list(target_tokenizer.word_counts.items())) \n",
        "target"
      ],
      "execution_count": 512,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>&lt;start&gt;</td>\n",
              "      <td>628500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>space</td>\n",
              "      <td>5865615</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>:</td>\n",
              "      <td>92871</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>&lt;end&gt;</td>\n",
              "      <td>628500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>.</td>\n",
              "      <td>95358</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>؛</td>\n",
              "      <td>41361</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>،</td>\n",
              "      <td>187018</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>؟</td>\n",
              "      <td>2770</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "         0        1\n",
              "0  <start>   628500\n",
              "1    space  5865615\n",
              "2        :    92871\n",
              "3    <end>   628500\n",
              "4        .    95358\n",
              "5        ؛    41361\n",
              "6        ،   187018\n",
              "7        ؟     2770"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 512
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gB1tRvqcKjvW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "outputId": "8dff923d-4a99-4774-feec-fcc5bac5c9ad"
      },
      "source": [
        "plt.xticks(fontsize =10)\n",
        "plt.bar(target[0],target[1], color=(0.2, 0.4, 0.6, 0.6))"
      ],
      "execution_count": 432,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<BarContainer object of 8 artists>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 432
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZAAAAD4CAYAAADCb7BPAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n/weDgVRLve4HUQAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "goYcNYDMe6Fq",
        "colab_type": "text"
      },
      "source": [
        "### Create dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Uv9ycnIeyueW",
        "colab": {}
      },
      "source": [
        "BUFFER_SIZE = len(input_tensor)\n",
        "BATCH_SIZE = 256\n",
        "steps_per_epoch = len(input_tensor)//BATCH_SIZE\n",
        "embedding_dim = 256\n",
        "units = 1024\n",
        "vocab_inp_size = len(input_tokenizer.word_index)+1\n",
        "vocab_tar_size = len(target_tokenizer.word_index)+1\n",
        "\n",
        "# create dataset\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices((input_tensor, target_tensor)).shuffle(BUFFER_SIZE)\n",
        "train_dataset = train_dataset.batch(BATCH_SIZE, drop_remainder=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "K1slax4qyueZ",
        "outputId": "4e7f3b57-2394-40f7-db6a-a59e759c3d16",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "example_input_batch, example_target_batch = next(iter(train_dataset))\n",
        "example_input_batch.shape, example_target_batch.shape"
      ],
      "execution_count": 224,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(TensorShape([Dimension(256), Dimension(12)]),\n",
              " TensorShape([Dimension(256), Dimension(12)]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 224
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NKzsyzvefDo2",
        "colab_type": "text"
      },
      "source": [
        "## Encoder and decoder model\n",
        "\n",
        "In this part of the code, we used the same technique of Neural Machine Translation provided by TensorFlow. However, we used Keras with the backend of TensorFlow. For more details can visit [Neural machine translation with attention tutorial](https://www.tensorflow.org/tutorials/text/nmt_with_attention)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "kEN7CY6lyued",
        "colab": {}
      },
      "source": [
        "class Encoder(tf.keras.Model):\n",
        "    def __init__(self, vocab_size, embedding_dim, enc_units, batch_sz):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.batch_sz = batch_sz\n",
        "        self.enc_units = enc_units\n",
        "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "        self.gru = tf.keras.layers.GRU(self.enc_units,\n",
        "                                       return_sequences=True,\n",
        "                                       return_state=True,\n",
        "                                       recurrent_initializer='glorot_uniform')\n",
        "\n",
        "    def call(self, x, hidden):\n",
        "        x = self.embedding(x)\n",
        "        output, state = self.gru(x, initial_state = hidden)\n",
        "        return output, state\n",
        "\n",
        "    def initialize_hidden_state(self):\n",
        "        return tf.zeros((self.batch_sz, self.enc_units))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ax5I6Vm8yuef",
        "outputId": "b91c9519-ca3a-4f9e-ed8a-56f64fb57b54",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "encoder = Encoder(vocab_inp_size, embedding_dim, units, BATCH_SIZE)\n",
        "\n",
        "# sample input\n",
        "sample_hidden = encoder.initialize_hidden_state()\n",
        "sample_output, sample_hidden = encoder(example_input_batch, sample_hidden)\n",
        "print ('Encoder output shape: (batch size, sequence length, units) {}'.format(sample_output.shape))\n",
        "print ('Encoder Hidden state shape: (batch size, units) {}'.format(sample_hidden.shape))"
      ],
      "execution_count": 226,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Encoder output shape: (batch size, sequence length, units) (256, 12, 1024)\n",
            "Encoder Hidden state shape: (batch size, units) (256, 1024)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "AKd0cO7Yyuei",
        "colab": {}
      },
      "source": [
        "class BahdanauAttention(tf.keras.layers.Layer):\n",
        "    def __init__(self, units):\n",
        "        super(BahdanauAttention, self).__init__()\n",
        "        self.W1 = tf.keras.layers.Dense(units)\n",
        "        self.W2 = tf.keras.layers.Dense(units)\n",
        "        self.V = tf.keras.layers.Dense(1)\n",
        "\n",
        "    def call(self, query, values):\n",
        "        # hidden shape == (batch_size, hidden size)\n",
        "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
        "        # we are doing this to perform addition to calculate the score\n",
        "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
        "\n",
        "        # score shape == (batch_size, max_length, 1)\n",
        "        # we get 1 at the last axis because we are applying score to self.V\n",
        "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
        "        score = self.V(tf.keras.layers.Activation(activation = \"tanh\")(tf.keras.layers.Add()([self.W1(values), self.W2(hidden_with_time_axis)])))    \n",
        "\n",
        "        # attention_weights shape == (batch_size, max_length, 1)\n",
        "        attention_weights = tf.keras.layers.Activation(activation = \"softmax\")(tf.keras.layers.Permute((2, 1))(score))\n",
        "        attention_weights = tf.keras.layers.Permute((2, 1))(attention_weights)\n",
        "\n",
        "        # context_vector shape after sum == (batch_size, hidden_size)\n",
        "        context_vector = tf.keras.layers.Multiply()([attention_weights, values])\n",
        "\n",
        "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
        "\n",
        "        return context_vector, attention_weights"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "2-rHOcxGyuem",
        "outputId": "d9b67924-3d0a-4ed1-e473-ea9fbfa8a5d4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "attention_layer = BahdanauAttention(10)\n",
        "attention_result, attention_weights = attention_layer(sample_hidden, sample_output)\n",
        "\n",
        "print(\"Attention result shape: (batch size, units) {}\".format(attention_result.shape))\n",
        "print(\"Attention weights shape: (batch_size, sequence_length, 1) {}\".format(attention_weights.shape))"
      ],
      "execution_count": 228,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Attention result shape: (batch size, units) (256, 1024)\n",
            "Attention weights shape: (batch_size, sequence_length, 1) (256, 12, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "IjCsCYq_yuep",
        "colab": {}
      },
      "source": [
        "class Decoder(tf.keras.Model):\n",
        "    def __init__(self, vocab_size, embedding_dim, dec_units, batch_sz):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.batch_sz = batch_sz\n",
        "        self.dec_units = dec_units\n",
        "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "        self.gru = tf.keras.layers.GRU(self.dec_units,\n",
        "                                       return_sequences=True,\n",
        "                                       return_state=True,\n",
        "                                       recurrent_initializer='glorot_uniform')\n",
        "        self.fc = tf.keras.layers.Dense(vocab_size)\n",
        "\n",
        "        # used for attention\n",
        "        self.attention = BahdanauAttention(self.dec_units)\n",
        "\n",
        "    def call(self, x, hidden, enc_output):\n",
        "        # enc_output shape == (batch_size, max_length, hidden_size)\n",
        "        context_vector, attention_weights = self.attention(hidden, enc_output)\n",
        "\n",
        "        # x shape after passing through embedding == (batch_size, 1, embedding_dim)\n",
        "        x = self.embedding(x)\n",
        "\n",
        "        # x shape after concatenation == (batch_size, 1, embedding_dim + hidden_size)\n",
        "        x = tf.keras.layers.Concatenate(axis = -1)([tf.expand_dims(context_vector, 1), x])\n",
        "\n",
        "        # passing the concatenated vector to the GRU\n",
        "        output, state = self.gru(x)\n",
        "\n",
        "        #output = tf.keras.layers.Dense(self.dec_units)(output)\n",
        "        # output shape == (batch_size * 1, hidden_size)\n",
        "        output = tf.keras.layers.Reshape((output.shape[2],))(output)\n",
        "\n",
        "        # output shape == (batch_size, vocab)\n",
        "        x = self.fc(output)\n",
        "\n",
        "        return x, state, attention_weights"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ehFo4Zq7yuer",
        "outputId": "5a92be09-fe8c-4ef2-c555-bafc074f27cb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "decoder = Decoder(vocab_tar_size, embedding_dim, units, BATCH_SIZE)\n",
        "\n",
        "sample_decoder_output, _, _ = decoder(tf.random.uniform((BATCH_SIZE, 1)),\n",
        "                                      sample_hidden, sample_output)\n",
        "\n",
        "print ('Decoder output shape: (batch_size, vocab size) {}'.format(sample_decoder_output.shape))"
      ],
      "execution_count": 240,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Decoder output shape: (batch_size, vocab size) (256, 11)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PcB91mNxg3L_",
        "colab_type": "text"
      },
      "source": [
        "### Optimizer and loss function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "4ib4lh5Hyueu",
        "colab": {}
      },
      "source": [
        "optimizer = tf.keras.optimizers.Adam()\n",
        "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
        "    from_logits=True, reduction='none')\n",
        "\n",
        "def loss_function(real, pred):\n",
        "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
        "    loss_ = loss_object(real, pred)\n",
        "\n",
        "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
        "    loss_ *= mask\n",
        "\n",
        "    return tf.reduce_mean(loss_)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "8pl0Bj4hyuew",
        "colab": {}
      },
      "source": [
        "checkpoint_dir = './training_checkpoints'\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
        "checkpoint = tf.train.Checkpoint(optimizer=optimizer,\n",
        "                                 encoder=encoder,\n",
        "                                 decoder=decoder)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Df6rittahBG-",
        "colab_type": "text"
      },
      "source": [
        "### Train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "J8XDHnUQyue0",
        "colab": {}
      },
      "source": [
        "@tf.function\n",
        "def train_step(inp, targ, enc_hidden):\n",
        "    '''\n",
        "    DESCRIPTION:\n",
        "    This function to train encode-decode model\n",
        "    INPUT: \n",
        "    inp: input vector\n",
        "    targ: target vector\n",
        "    enc_hidden: encoder initial hidden state\n",
        "    OUTPUT: \n",
        "    batch_loss: train loss\n",
        "    ''' \n",
        "\n",
        "    loss = 0\n",
        "\n",
        "    with tf.GradientTape() as tape:\n",
        "        enc_output, enc_hidden = encoder(inp, enc_hidden)\n",
        "\n",
        "        dec_hidden = enc_hidden\n",
        "\n",
        "        dec_input = tf.expand_dims([target_tokenizer.word_index['<start>']] * BATCH_SIZE, 1)\n",
        "\n",
        "        # Teacher forcing - feeding the target as the next input\n",
        "        for t in range(1, targ.shape[1]):\n",
        "          # passing enc_output to the decoder\n",
        "          predictions, dec_hidden, _ = decoder(dec_input, dec_hidden, enc_output)\n",
        "\n",
        "          loss += loss_function(targ[:, t], predictions)\n",
        "\n",
        "          # using teacher forcing\n",
        "          dec_input = tf.expand_dims(targ[:, t], 1)\n",
        "\n",
        "    batch_loss = (loss / int(targ.shape[1]))\n",
        "\n",
        "    variables = encoder.trainable_variables + decoder.trainable_variables\n",
        "\n",
        "    gradients = tape.gradient(loss, variables)\n",
        "\n",
        "    optimizer.apply_gradients(zip(gradients, variables))\n",
        "\n",
        "    return batch_loss\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3wA2-LY_A-xt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "1374c183-b9be-4823-b710-fead6b001a86"
      },
      "source": [
        "EPOCHS = 10\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "  start = time.time()\n",
        "\n",
        "  enc_hidden = encoder.initialize_hidden_state()\n",
        "  total_loss = 0\n",
        "\n",
        "  for (batch, (inp, targ)) in enumerate(train_dataset.take(steps_per_epoch)):\n",
        "    batch_loss = train_step(inp, targ, enc_hidden)\n",
        "    total_loss += batch_loss\n",
        "\n",
        "    if batch % 100 == 0:\n",
        "      print('Epoch {} Batch {} Loss {:.4f}'.format(epoch + 1,\n",
        "                                                   batch,\n",
        "                                                   batch_loss.numpy()))\n",
        "  # saving (checkpoint) the model every 2 epochs\n",
        "  if (epoch + 1) % 2 == 0:\n",
        "    checkpoint.save(file_prefix = checkpoint_prefix)\n",
        "\n",
        "  print('Epoch {} Loss {:.4f}'.format(epoch + 1,\n",
        "                                      total_loss / steps_per_epoch))\n",
        "  print('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))"
      ],
      "execution_count": 244,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1 Batch 0 Loss 2.2024\n",
            "Epoch 1 Batch 100 Loss 0.5358\n",
            "Epoch 1 Batch 200 Loss 0.5134\n",
            "Epoch 1 Batch 300 Loss 0.5058\n",
            "Epoch 1 Batch 400 Loss 0.4831\n",
            "Epoch 1 Batch 500 Loss 0.4823\n",
            "Epoch 1 Batch 600 Loss 0.4879\n",
            "Epoch 1 Batch 700 Loss 0.4299\n",
            "Epoch 1 Batch 800 Loss 0.4163\n",
            "Epoch 1 Batch 900 Loss 0.3736\n",
            "Epoch 1 Batch 1000 Loss 0.2317\n",
            "Epoch 1 Batch 1100 Loss 0.1912\n",
            "Epoch 1 Batch 1200 Loss 0.1542\n",
            "Epoch 1 Batch 1300 Loss 0.1290\n",
            "Epoch 1 Batch 1400 Loss 0.1254\n",
            "Epoch 1 Batch 1500 Loss 0.1616\n",
            "Epoch 1 Batch 1600 Loss 0.1669\n",
            "Epoch 1 Batch 1700 Loss 0.4251\n",
            "Epoch 1 Batch 1800 Loss 0.4228\n",
            "Epoch 1 Batch 1900 Loss 0.1847\n",
            "Epoch 1 Batch 2000 Loss 0.1584\n",
            "Epoch 1 Batch 2100 Loss 0.1351\n",
            "Epoch 1 Batch 2200 Loss 0.1359\n",
            "Epoch 1 Batch 2300 Loss 0.4841\n",
            "Epoch 1 Batch 2400 Loss 0.4762\n",
            "Epoch 1 Loss 0.3359\n",
            "Time taken for 1 epoch 1180.6459941864014 sec\n",
            "\n",
            "Epoch 2 Batch 0 Loss 0.4603\n",
            "Epoch 2 Batch 100 Loss 0.4961\n",
            "Epoch 2 Batch 200 Loss 0.3089\n",
            "Epoch 2 Batch 300 Loss 0.1461\n",
            "Epoch 2 Batch 400 Loss 0.1429\n",
            "Epoch 2 Batch 500 Loss 0.1557\n",
            "Epoch 2 Batch 600 Loss 0.5531\n",
            "Epoch 2 Batch 700 Loss 0.5135\n",
            "Epoch 2 Batch 800 Loss 0.4721\n",
            "Epoch 2 Batch 900 Loss 0.5182\n",
            "Epoch 2 Batch 1000 Loss 0.2640\n",
            "Epoch 2 Batch 1100 Loss 0.5230\n",
            "Epoch 2 Batch 1200 Loss 0.3047\n",
            "Epoch 2 Batch 1300 Loss 0.3803\n",
            "Epoch 2 Batch 1400 Loss 0.1588\n",
            "Epoch 2 Batch 1500 Loss 0.1659\n",
            "Epoch 2 Batch 1600 Loss 0.1647\n",
            "Epoch 2 Batch 1700 Loss 0.1318\n",
            "Epoch 2 Batch 1800 Loss 0.5022\n",
            "Epoch 2 Batch 1900 Loss 0.4681\n",
            "Epoch 2 Batch 2000 Loss 0.2305\n",
            "Epoch 2 Batch 2100 Loss 0.1364\n",
            "Epoch 2 Batch 2200 Loss 0.1322\n",
            "Epoch 2 Batch 2300 Loss 0.1489\n",
            "Epoch 2 Batch 2400 Loss 0.1433\n",
            "Epoch 2 Loss 0.2998\n",
            "Time taken for 1 epoch 1151.6674664020538 sec\n",
            "\n",
            "Epoch 3 Batch 0 Loss 0.1461\n",
            "Epoch 3 Batch 100 Loss 0.1400\n",
            "Epoch 3 Batch 200 Loss 0.2134\n",
            "Epoch 3 Batch 300 Loss 0.2771\n",
            "Epoch 3 Batch 400 Loss 0.1381\n",
            "Epoch 3 Batch 500 Loss 0.1385\n",
            "Epoch 3 Batch 600 Loss 0.2567\n",
            "Epoch 3 Batch 700 Loss 0.1340\n",
            "Epoch 3 Batch 800 Loss 0.1546\n",
            "Epoch 3 Batch 900 Loss 0.1473\n",
            "Epoch 3 Batch 1000 Loss 0.1293\n",
            "Epoch 3 Batch 1100 Loss 0.1607\n",
            "Epoch 3 Batch 1200 Loss 0.1372\n",
            "Epoch 3 Batch 1300 Loss 0.1163\n",
            "Epoch 3 Batch 1400 Loss 0.5083\n",
            "Epoch 3 Batch 1500 Loss 0.4989\n",
            "Epoch 3 Batch 1600 Loss 0.4925\n",
            "Epoch 3 Batch 1700 Loss 0.3468\n",
            "Epoch 3 Batch 1800 Loss 0.4830\n",
            "Epoch 3 Batch 1900 Loss 0.1942\n",
            "Epoch 3 Batch 2000 Loss 0.1588\n",
            "Epoch 3 Batch 2100 Loss 0.1335\n",
            "Epoch 3 Batch 2200 Loss 0.1309\n",
            "Epoch 3 Batch 2300 Loss 0.1443\n",
            "Epoch 3 Batch 2400 Loss 0.1413\n",
            "Epoch 3 Loss 0.2289\n",
            "Time taken for 1 epoch 1150.04203748703 sec\n",
            "\n",
            "Epoch 4 Batch 0 Loss 0.1443\n",
            "Epoch 4 Batch 100 Loss 0.1621\n",
            "Epoch 4 Batch 200 Loss 0.1409\n",
            "Epoch 4 Batch 300 Loss 0.1333\n",
            "Epoch 4 Batch 400 Loss 0.1297\n",
            "Epoch 4 Batch 500 Loss 0.1323\n",
            "Epoch 4 Batch 600 Loss 0.1450\n",
            "Epoch 4 Batch 700 Loss 0.1277\n",
            "Epoch 4 Batch 800 Loss 0.1489\n",
            "Epoch 4 Batch 900 Loss 0.1456\n",
            "Epoch 4 Batch 1000 Loss 0.1267\n",
            "Epoch 4 Batch 1100 Loss 0.1572\n",
            "Epoch 4 Batch 1200 Loss 0.1335\n",
            "Epoch 4 Batch 1300 Loss 0.1156\n",
            "Epoch 4 Batch 1400 Loss 0.1839\n",
            "Epoch 4 Batch 1500 Loss 0.1554\n",
            "Epoch 4 Batch 1600 Loss 0.1543\n",
            "Epoch 4 Batch 1700 Loss 0.1302\n",
            "Epoch 4 Batch 1800 Loss 0.1292\n",
            "Epoch 4 Batch 1900 Loss 0.1523\n",
            "Epoch 4 Batch 2000 Loss 0.1391\n",
            "Epoch 4 Batch 2100 Loss 0.1255\n",
            "Epoch 4 Batch 2200 Loss 0.1286\n",
            "Epoch 4 Batch 2300 Loss 0.1380\n",
            "Epoch 4 Batch 2400 Loss 0.1357\n",
            "Epoch 4 Loss 0.1421\n",
            "Time taken for 1 epoch 1150.3518314361572 sec\n",
            "\n",
            "Epoch 5 Batch 0 Loss 0.1371\n",
            "Epoch 5 Batch 100 Loss 0.1357\n",
            "Epoch 5 Batch 200 Loss 0.1352\n",
            "Epoch 5 Batch 300 Loss 0.1299\n",
            "Epoch 5 Batch 400 Loss 0.1260\n",
            "Epoch 5 Batch 500 Loss 0.1287\n",
            "Epoch 5 Batch 600 Loss 0.1416\n",
            "Epoch 5 Batch 700 Loss 0.1239\n",
            "Epoch 5 Batch 800 Loss 0.1446\n",
            "Epoch 5 Batch 900 Loss 0.1432\n",
            "Epoch 5 Batch 1000 Loss 0.3703\n",
            "Epoch 5 Batch 1100 Loss 0.1699\n",
            "Epoch 5 Batch 1200 Loss 0.1368\n",
            "Epoch 5 Batch 1300 Loss 0.1152\n",
            "Epoch 5 Batch 1400 Loss 0.1211\n",
            "Epoch 5 Batch 1500 Loss 0.1507\n",
            "Epoch 5 Batch 1600 Loss 0.1517\n",
            "Epoch 5 Batch 1700 Loss 0.1286\n",
            "Epoch 5 Batch 1800 Loss 0.1286\n",
            "Epoch 5 Batch 1900 Loss 0.1337\n",
            "Epoch 5 Batch 2000 Loss 0.1371\n",
            "Epoch 5 Batch 2100 Loss 0.1229\n",
            "Epoch 5 Batch 2200 Loss 0.1261\n",
            "Epoch 5 Batch 2300 Loss 0.1363\n",
            "Epoch 5 Batch 2400 Loss 0.1317\n",
            "Epoch 5 Loss 0.1462\n",
            "Time taken for 1 epoch 1145.7994616031647 sec\n",
            "\n",
            "Epoch 6 Batch 0 Loss 0.1355\n",
            "Epoch 6 Batch 100 Loss 0.1327\n",
            "Epoch 6 Batch 200 Loss 0.1338\n",
            "Epoch 6 Batch 300 Loss 0.1290\n",
            "Epoch 6 Batch 400 Loss 0.3524\n",
            "Epoch 6 Batch 500 Loss 0.3564\n",
            "Epoch 6 Batch 600 Loss 0.1466\n",
            "Epoch 6 Batch 700 Loss 0.1266\n",
            "Epoch 6 Batch 800 Loss 0.1456\n",
            "Epoch 6 Batch 900 Loss 0.2993\n",
            "Epoch 6 Batch 1000 Loss 0.1302\n",
            "Epoch 6 Batch 1100 Loss 0.1576\n",
            "Epoch 6 Batch 1200 Loss 0.1715\n",
            "Epoch 6 Batch 1300 Loss 0.1280\n",
            "Epoch 6 Batch 1400 Loss 0.1192\n",
            "Epoch 6 Batch 1500 Loss 0.1529\n",
            "Epoch 6 Batch 1600 Loss 0.1519\n",
            "Epoch 6 Batch 1700 Loss 0.1275\n",
            "Epoch 6 Batch 1800 Loss 0.1260\n",
            "Epoch 6 Batch 1900 Loss 0.1343\n",
            "Epoch 6 Batch 2000 Loss 0.1342\n",
            "Epoch 6 Batch 2100 Loss 0.1216\n",
            "Epoch 6 Batch 2200 Loss 0.1234\n",
            "Epoch 6 Batch 2300 Loss 0.1338\n",
            "Epoch 6 Batch 2400 Loss 0.1279\n",
            "Epoch 6 Loss 0.1691\n",
            "Time taken for 1 epoch 1147.9241325855255 sec\n",
            "\n",
            "Epoch 7 Batch 0 Loss 0.1341\n",
            "Epoch 7 Batch 100 Loss 0.1318\n",
            "Epoch 7 Batch 200 Loss 0.1328\n",
            "Epoch 7 Batch 300 Loss 0.1266\n",
            "Epoch 7 Batch 400 Loss 0.1245\n",
            "Epoch 7 Batch 500 Loss 0.1478\n",
            "Epoch 7 Batch 600 Loss 0.1404\n",
            "Epoch 7 Batch 700 Loss 0.1262\n",
            "Epoch 7 Batch 800 Loss 0.1422\n",
            "Epoch 7 Batch 900 Loss 0.1367\n",
            "Epoch 7 Batch 1000 Loss 0.1240\n",
            "Epoch 7 Batch 1100 Loss 0.1499\n",
            "Epoch 7 Batch 1200 Loss 0.1290\n",
            "Epoch 7 Batch 1300 Loss 0.1115\n",
            "Epoch 7 Batch 1400 Loss 0.1118\n",
            "Epoch 7 Batch 1500 Loss 0.1464\n",
            "Epoch 7 Batch 1600 Loss 0.1465\n",
            "Epoch 7 Batch 1700 Loss 0.1249\n",
            "Epoch 7 Batch 1800 Loss 0.1234\n",
            "Epoch 7 Batch 1900 Loss 0.1293\n",
            "Epoch 7 Batch 2000 Loss 0.1335\n",
            "Epoch 7 Batch 2100 Loss 0.1174\n",
            "Epoch 7 Batch 2200 Loss 0.1211\n",
            "Epoch 7 Batch 2300 Loss 0.3789\n",
            "Epoch 7 Batch 2400 Loss 0.2104\n",
            "Epoch 7 Loss 0.1513\n",
            "Time taken for 1 epoch 1147.0179541110992 sec\n",
            "\n",
            "Epoch 8 Batch 0 Loss 0.1500\n",
            "Epoch 8 Batch 100 Loss 0.1346\n",
            "Epoch 8 Batch 200 Loss 0.1302\n",
            "Epoch 8 Batch 300 Loss 0.1230\n",
            "Epoch 8 Batch 400 Loss 0.1221\n",
            "Epoch 8 Batch 500 Loss 0.1319\n",
            "Epoch 8 Batch 600 Loss 0.1327\n",
            "Epoch 8 Batch 700 Loss 0.1243\n",
            "Epoch 8 Batch 800 Loss 0.1381\n",
            "Epoch 8 Batch 900 Loss 0.1346\n",
            "Epoch 8 Batch 1000 Loss 0.1215\n",
            "Epoch 8 Batch 1100 Loss 0.1504\n",
            "Epoch 8 Batch 1200 Loss 0.1280\n",
            "Epoch 8 Batch 1300 Loss 0.1087\n",
            "Epoch 8 Batch 1400 Loss 0.1114\n",
            "Epoch 8 Batch 1500 Loss 0.1442\n",
            "Epoch 8 Batch 1600 Loss 0.1428\n",
            "Epoch 8 Batch 1700 Loss 0.1242\n",
            "Epoch 8 Batch 1800 Loss 0.1211\n",
            "Epoch 8 Batch 1900 Loss 0.1247\n",
            "Epoch 8 Batch 2000 Loss 0.1307\n",
            "Epoch 8 Batch 2100 Loss 0.1153\n",
            "Epoch 8 Batch 2200 Loss 0.1171\n",
            "Epoch 8 Batch 2300 Loss 0.1281\n",
            "Epoch 8 Batch 2400 Loss 0.1241\n",
            "Epoch 8 Loss 0.1279\n",
            "Time taken for 1 epoch 1151.0571682453156 sec\n",
            "\n",
            "Epoch 9 Batch 0 Loss 0.1302\n",
            "Epoch 9 Batch 100 Loss 0.1261\n",
            "Epoch 9 Batch 200 Loss 0.1261\n",
            "Epoch 9 Batch 300 Loss 0.1282\n",
            "Epoch 9 Batch 400 Loss 0.1163\n",
            "Epoch 9 Batch 500 Loss 0.1269\n",
            "Epoch 9 Batch 600 Loss 0.1286\n",
            "Epoch 9 Batch 700 Loss 0.1245\n",
            "Epoch 9 Batch 800 Loss 0.1329\n",
            "Epoch 9 Batch 900 Loss 0.1332\n",
            "Epoch 9 Batch 1000 Loss 0.1267\n",
            "Epoch 9 Batch 1100 Loss 0.3060\n",
            "Epoch 9 Batch 1200 Loss 0.1262\n",
            "Epoch 9 Batch 1300 Loss 0.1095\n",
            "Epoch 9 Batch 1400 Loss 0.1107\n",
            "Epoch 9 Batch 1500 Loss 0.1445\n",
            "Epoch 9 Batch 1600 Loss 0.1434\n",
            "Epoch 9 Batch 1700 Loss 0.1222\n",
            "Epoch 9 Batch 1800 Loss 0.1215\n",
            "Epoch 9 Batch 1900 Loss 0.1237\n",
            "Epoch 9 Batch 2000 Loss 0.1314\n",
            "Epoch 9 Batch 2100 Loss 0.1116\n",
            "Epoch 9 Batch 2200 Loss 0.1141\n",
            "Epoch 9 Batch 2300 Loss 0.1255\n",
            "Epoch 9 Batch 2400 Loss 0.4515\n",
            "Epoch 9 Loss 0.1569\n",
            "Time taken for 1 epoch 1150.1276350021362 sec\n",
            "\n",
            "Epoch 10 Batch 0 Loss 0.4091\n",
            "Epoch 10 Batch 100 Loss 0.2484\n",
            "Epoch 10 Batch 200 Loss 0.1915\n",
            "Epoch 10 Batch 300 Loss 0.1583\n",
            "Epoch 10 Batch 400 Loss 0.1432\n",
            "Epoch 10 Batch 500 Loss 0.1424\n",
            "Epoch 10 Batch 600 Loss 0.1500\n",
            "Epoch 10 Batch 700 Loss 0.1330\n",
            "Epoch 10 Batch 800 Loss 0.1434\n",
            "Epoch 10 Batch 900 Loss 0.1442\n",
            "Epoch 10 Batch 1000 Loss 0.1252\n",
            "Epoch 10 Batch 1100 Loss 0.1540\n",
            "Epoch 10 Batch 1200 Loss 0.1278\n",
            "Epoch 10 Batch 1300 Loss 0.1127\n",
            "Epoch 10 Batch 1400 Loss 0.1139\n",
            "Epoch 10 Batch 1500 Loss 0.1455\n",
            "Epoch 10 Batch 1600 Loss 0.1451\n",
            "Epoch 10 Batch 1700 Loss 0.1242\n",
            "Epoch 10 Batch 1800 Loss 0.1234\n",
            "Epoch 10 Batch 1900 Loss 0.3803\n",
            "Epoch 10 Batch 2000 Loss 0.3950\n",
            "Epoch 10 Batch 2100 Loss 0.3271\n",
            "Epoch 10 Batch 2200 Loss 0.1844\n",
            "Epoch 10 Batch 2300 Loss 0.1689\n",
            "Epoch 10 Batch 2400 Loss 0.1684\n",
            "Epoch 10 Loss 0.1844\n",
            "Time taken for 1 epoch 1154.2842366695404 sec\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pgfHSUhgiETN",
        "colab_type": "text"
      },
      "source": [
        "### Predict"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "qIT8TVVpyue7",
        "colab": {}
      },
      "source": [
        "def evaluate(sentence):\n",
        "    '''\n",
        "    DESCRIPTION:\n",
        "    This function to predict result\n",
        "    INPUT: \n",
        "    sentence: input sentence \n",
        "    OUTPUT: \n",
        "    result: predict result\n",
        "    sentence: input sentence \n",
        "    attention_plot: attention weights\n",
        "    ''' \n",
        "\n",
        "  attention_plot = np.zeros((target_max_length, input_max_length))\n",
        "\n",
        "  #sentence = preprocess_sentence(sentence)\n",
        "  sentence = sentence + ' <end>'\n",
        "  inputs = [input_tokenizer.texts_to_sequences([i])[0][0] for i in sentence.split(' ')]\n",
        "  inputs = tf.keras.preprocessing.sequence.pad_sequences([inputs],\n",
        "                                                         maxlen=input_max_length,\n",
        "                                                         padding='post')\n",
        "  inputs = tf.convert_to_tensor(inputs)\n",
        "\n",
        "  result = ''\n",
        "\n",
        "  hidden = [tf.zeros((1, units))]\n",
        "  enc_out, enc_hidden = encoder(inputs, hidden)\n",
        "\n",
        "  dec_hidden = enc_hidden\n",
        "  dec_input = tf.expand_dims([target_tokenizer.word_index['<start>']], 0)\n",
        "\n",
        "  for t in range(target_max_length):\n",
        "    predictions, dec_hidden, attention_weights = decoder(dec_input,\n",
        "                                                         dec_hidden,\n",
        "                                                         enc_out)\n",
        "\n",
        "    # storing the attention weights to plot later on\n",
        "    attention_weights = tf.reshape(attention_weights, (-1, ))\n",
        "    attention_plot[t] = attention_weights.numpy()\n",
        "\n",
        "    predicted_id = tf.argmax(predictions[0]).numpy()\n",
        "\n",
        "    result += target_tokenizer.index_word[predicted_id] + ' '\n",
        "\n",
        "    if target_tokenizer.index_word[predicted_id] == '<end>':\n",
        "      return result, sentence, attention_plot\n",
        "\n",
        "    # the predicted ID is fed back into the model\n",
        "    dec_input = tf.expand_dims([predicted_id], 0)\n",
        "\n",
        "  return result, sentence, attention_plot"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "C7J0Y5qoyue9",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "wSpX2iMB3Gfl",
        "colab": {}
      },
      "source": [
        "# function for plotting the attention weights\n",
        "def plot_attention(attention, sentence, predicted_sentence):\n",
        "  '''\n",
        "    DESCRIPTION:\n",
        "    This function to plot attention \n",
        "    INPUT: \n",
        "    attention: attention weights\n",
        "    sentence: input sentence \n",
        "    predicted_sentence: predict result\n",
        "\n",
        "    OUTPUT: \n",
        "    None\n",
        "    ''' \n",
        "  fig = plt.figure(figsize=(10,10))\n",
        "  ax = fig.add_subplot(1, 1, 1)\n",
        "  ax.matshow(attention, cmap='viridis')\n",
        "\n",
        "  fontdict = {'fontsize': 14}\n",
        "\n",
        "  ax.set_xticklabels([''] + sentence, fontdict=fontdict, rotation=90)\n",
        "  ax.set_yticklabels([''] + predicted_sentence, fontdict=fontdict)\n",
        "\n",
        "  ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
        "  ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
        "\n",
        "  plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "EX-v514W3GlB",
        "colab": {}
      },
      "source": [
        "def predict(sentence):\n",
        "    '''\n",
        "    DESCRIPTION:\n",
        "    This function to predict output sentence\n",
        "\n",
        "    INPUT: \n",
        "    sentence: input sentence \n",
        "\n",
        "    OUTPUT: \n",
        "    None\n",
        "    ''' \n",
        "    result, sentence, attention_plot = evaluate(sentence)\n",
        "\n",
        "    print('Input:\\n %s' % (sentence))\n",
        "\n",
        "    predict = ''\n",
        "    sentence_list = sentence.split(' ')\n",
        "    result_list = result.split(' ')\n",
        "\n",
        "    for i in range(len(sentence_list)-1):\n",
        "      if (result_list[i]=='space'):\n",
        "        predict += sentence_list[i]+' '\n",
        "      else:\n",
        "        predict += sentence_list[i]+result_list[i]+' '\n",
        "    print('Predicted punctuation:\\n {}'.format(predict))\n",
        "\n",
        "    attention_plot = attention_plot[:len(result.split(' ')), :len(sentence.split(' '))]\n",
        "    plot_attention(attention_plot, sentence.split(' '), result.split(' '))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "fFj-3K0Ryue_",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 733
        },
        "outputId": "b41ec3f3-3fe6-4b42-cf2b-a0855ea0a01b"
      },
      "source": [
        "\n",
        "sentence = 'قال تجنب الكهرباء لأنها خطره'\n",
        "\n",
        "in_seq = sentence.strip().split(' ')\n",
        "n =10\n",
        "in_sequances = [\" \".join(in_seq[i:i+n]) for i in range(0, len(in_seq), n)]\n",
        "[predict(s) for s in in_sequances]"
      ],
      "execution_count": 519,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input:\n",
            " قال تجنب الكهرباء لأنها خطره <end>\n",
            "Predicted punctuation:\n",
            " قال: تجنب الكهرباء لأنها خطره \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAU4AAAJ4CAYAAAAKmApEAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nZpn+f+Vv0cqfcQByAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 720x720 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[None]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 519
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "KUZpSxWSyufB",
        "colab": {}
      },
      "source": [
        "#save the encoder decoder\n",
        "encoder.save_weights('encoder')\n",
        "decoder.save_weights('decoder')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "8ELp47DUyufE",
        "colab": {}
      },
      "source": [
        "#save the dictionary\n",
        "import csv\n",
        "def create_csv(file, dict):\n",
        "    with open(file, 'w') as csvfile:\n",
        "        writer = csv.writer(csvfile)\n",
        "        for key in dict.keys():\n",
        "            writer.writerow([key,dict[key]])\n",
        "create_csv('idx2word_in.csv', input_tokenizer.index_word)\n",
        "create_csv('idx2word_out.csv', target_tokenizer.index_word)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "HHK_3-LXyufL",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "VN0UHFxyyufO",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}